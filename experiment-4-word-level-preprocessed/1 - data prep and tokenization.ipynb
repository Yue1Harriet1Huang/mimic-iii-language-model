{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 - Word-level LM with Sampled Data, Preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mimic3-utils (https://github.com/sudarshan85/mimic3-utils/) the data has been preprocessed to replace commonly redacted information with tokens. \n",
    "\n",
    "The following notebook will allow you to get to the same processed data that I load below:\n",
    "https://github.com/sudarshan85/mimic3-utils/blob/master/mimic-notes-preprocess-from-file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('/home/paperspace/data/mimic-iii')\n",
    "LM_PATH=PATH/'exp-4'\n",
    "\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train.csv', chunksize=chunksize, engine='python')\n",
    "df_val = pd.read_csv(LM_PATH/'val.csv', chunksize=chunksize, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df):\n",
    "    texts = f'\\n{BOS} ' + df.proc_text.astype(str)\n",
    "    tok = Tokenizer.proc_all_mp(partition_by_cores(texts), lang='en')\n",
    "    return tok\n",
    "\n",
    "def get_all(df, name):\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_  = get_texts(r)\n",
    "        #save the partial tokens instead of regrouping them in one big array.\n",
    "        np.save(LM_PATH/f'{name}_tok{i}.npy', tok_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "get_all(df_trn,'trn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "get_all(df_val,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_them_all(names):\n",
    "    cnt = Counter()\n",
    "    for name in names:\n",
    "        for file in LM_PATH.glob(f'{name}_tok*'):\n",
    "            tok = np.load(file)\n",
    "            cnt_tok = Counter(word for sent in tok for word in sent)\n",
    "            cnt += cnt_tok\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = count_them_all(['trn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n ', 34164786),\n",
       " ('t_up', 30174417),\n",
       " ('.', 24853159),\n",
       " (':', 17153030),\n",
       " (',', 14379149),\n",
       " ('\\n', 12111097),\n",
       " ('/', 9688055),\n",
       " ('and', 6589993),\n",
       " ('to', 5991476),\n",
       " ('the', 5908185),\n",
       " ('-', 5541930),\n",
       " ('of', 5245754),\n",
       " ('with', 4603970),\n",
       " (')', 4480676),\n",
       " ('(', 3975311),\n",
       " ('for', 3420196),\n",
       " ('in', 3383370),\n",
       " ('xxdate', 3099099),\n",
       " ('is', 2993496),\n",
       " ('on', 2913167),\n",
       " ('no', 2875740),\n",
       " ('patient', 2621691),\n",
       " ('\\n\\n ', 2423244),\n",
       " ('a', 2193193),\n",
       " ('\\n\\n', 2170884),\n",
       " ('was', 1947243),\n",
       " ('at', 1759823),\n",
       " ('mg', 1480230),\n",
       " ('left', 1378568),\n",
       " ('%', 1347992),\n",
       " ('xbos', 1328353),\n",
       " ('this', 1313900),\n",
       " ('as', 1291006),\n",
       " ('#', 1264849),\n",
       " ('right', 1264339),\n",
       " ('ml', 1185811),\n",
       " ('not', 1044926),\n",
       " ('xxmmdd', 989587),\n",
       " ('there', 987744),\n",
       " ('are', 985142),\n",
       " ('or', 984127),\n",
       " ('1', 982599),\n",
       " ('2', 972217),\n",
       " ('s', 935815),\n",
       " ('tk_rep', 929212),\n",
       " (';', 917933),\n",
       " ('p', 880212),\n",
       " ('*', 869060),\n",
       " ('from', 861849),\n",
       " ('chest', 815991)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.most_common(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in cnt.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0,'_pad_')\n",
    "itos.insert(0,'_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(int,{s:i for (i,s) in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(LM_PATH/'stoi.pickle','rb') as f:\n",
    "#    stoi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(name, partial=True):\n",
    "    results = []\n",
    "    for index, file in enumerate(LM_PATH.glob(f'{name}_tok*')):\n",
    "        print(index)\n",
    "        tok = np.load(file)\n",
    "        results.append(np.array([[stoi[word] for word in sent] for sent in tok]))\n",
    "        \n",
    "        if (index == 10) and (partial==True):\n",
    "            break\n",
    "\n",
    "    return np.concatenate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "trn_ids = numericalize('trn')\n",
    "np.save(LM_PATH/'trn_ids.npy', trn_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "val_ids = numericalize('val')\n",
    "np.save(LM_PATH/'val_ids.npy', val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LM_PATH/'itos.pickle', 'wb') as handle:\n",
    "    pickle.dump(itos, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LM_PATH/'stoi.pickle', 'wb') as handle:\n",
    "    pickle.dump(stoi, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ids = np.load(LM_PATH/'trn_ids.npy')\n",
    "np.save(LM_PATH/'trn_ids_concat.npy', np.concatenate(trn_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = np.load(LM_PATH/'val_ids.npy')\n",
    "np.save(LM_PATH/'val_ids_concat.npy', np.concatenate(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
